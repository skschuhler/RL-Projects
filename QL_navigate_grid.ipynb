{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modified version of ALucek's tutorial at https://www.youtube.com/watch?v=qTY4Rr-x5q0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start Position: (0,0)\n",
    "- Obstacles: (1,0), (0,3), (2,1), (3,1), (4,3)\n",
    "- End Position: (5,5)\n",
    "\n",
    "Possible Actions: Up, down, left, right\n",
    "\n",
    " - Objective 1: reach goal tile\n",
    " - Objective 2: find most efficient path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world\n",
    "GRID_SIZE = 5\n",
    "START = (0, 0)\n",
    "GOAL = (4, 4)\n",
    "OBSTACLES = [(1, 0), (0, 3), (2, 1), (3, 1), (4, 3)]\n",
    "\n",
    "# Define actions\n",
    "ACTIONS = [\n",
    "    (-1, 0),  # up\n",
    "    (0, 1),   # right\n",
    "    (1, 0),   # down\n",
    "    (0, -1)   # left\n",
    "]\n",
    "\n",
    "# returns True if state is within grid boundaries, False if not\n",
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and \n",
    "            0 <= state[1] < GRID_SIZE and \n",
    "            state not in OBSTACLES)\n",
    "\n",
    "# adds action to current state and checks if it's valid\n",
    "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple[int, int]:\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    return next_state if is_valid_state(next_state) else state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Q-Learning parameters\n",
    "EPSILON = 0.3 # exploration rate\n",
    "ALPHA = 0.3 # learning rate\n",
    "GAMMA = 0.99 # discount factor (long-term rewards vs. short-term rewards)\n",
    "EPISODES = 10000 # number of full runs through the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif next_state in OBSTACLES or next_state == state:  # Penalize hitting walls or obstacles\n",
    "        return -10\n",
    "    else:\n",
    "        return -1  # Small penalty for each step to encourage shortest path\n",
    "    \n",
    "\n",
    "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
    "    if random.uniform(0, 1) < EPSILON: #epsilon-greedy strategy: choose random action EPSILON% of the time\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]\n",
    "    \n",
    "def update_q_table(q_table: np.ndarray, state: Tuple[int, int], action: Tuple[int, int], \n",
    "                   reward: int, next_state: Tuple[int, int]) -> None:\n",
    "    action_idx = ACTIONS.index(action)\n",
    "    q_table[state][action_idx] += ALPHA * (reward + GAMMA * np.max(q_table[next_state]) - q_table[state][action_idx]) # Q-value for taking action A in state S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent() -> np.ndarray:\n",
    "    q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "    \n",
    "    for _ in range(EPISODES):\n",
    "        state = START\n",
    "        while state != GOAL:\n",
    "            action = choose_action(state, q_table)\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = get_reward(state, next_state)\n",
    "            update_q_table(q_table, state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "# Train the agent\n",
    "q_table = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Q-table Grid:\n",
      "   |   (0,0)   |   (0,1)   |   (0,2)   |   (0,3)   |   (0,4)   |   (1,0)   |   (1,1)   |   (1,2)   |   (1,3)   |   (1,4)   |   (2,0)   |   (2,1)   |   (2,2)   |   (2,3)   |   (2,4)   |   (3,0)   |   (3,1)   |   (3,2)   |   (3,3)   |   (3,4)   |   (4,0)   |   (4,1)   |   (4,2)   |   (4,3)   |   (4,4)   |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " ^ |     75.55 |     77.41 |     79.30 | OBSTACLE  |     83.12 | OBSTACLE  |     86.41 |     88.30 |     83.12 |     92.12 |      0.00 | OBSTACLE  |     90.20 |     92.12 |     94.06 |      0.00 | OBSTACLE  |     92.12 |     94.06 |     96.02 |      0.00 |     -3.00 |     94.06 | OBSTACLE  |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " > |     86.41 |     88.30 |     79.30 | OBSTACLE  |     83.12 | OBSTACLE  |     90.20 |     92.12 |     94.06 |     85.06 |      0.00 | OBSTACLE  |     94.06 |     96.02 |     87.02 |      0.00 | OBSTACLE  |     96.02 |     98.00 |     89.00 |      0.00 |     62.80 |     65.83 | OBSTACLE  |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " v |     75.55 |     88.30 |     90.20 | OBSTACLE  |     94.06 | OBSTACLE  |     79.30 |     92.12 |     94.06 |     96.02 |      0.00 | OBSTACLE  |     94.06 |     96.02 |     98.00 |      0.00 | OBSTACLE  |     92.10 |     87.02 |    100.00 |      0.00 |      0.00 |     24.37 | OBSTACLE  |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " < |     75.55 |     84.55 |     86.41 | OBSTACLE  |     83.12 | OBSTACLE  |     79.30 |     88.30 |     90.20 |     92.12 |      0.00 | OBSTACLE  |     83.12 |     92.12 |     94.06 |      0.00 | OBSTACLE  |     85.06 |     94.06 |     96.02 |      0.00 |      0.00 |     22.10 | OBSTACLE  |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best Actions Grid:\n",
      "-----------------------------------------------------------------------\n",
      "| >:  86.41   | >:  88.30   | v:  90.20   |  OBSTACLE   | v:  94.06   | \n",
      "-----------------------------------------------------------------------\n",
      "|  OBSTACLE   | >:  90.20   | >:  92.12   | >:  94.06   | v:  96.02   | \n",
      "-----------------------------------------------------------------------\n",
      "| ^:   0.00   |  OBSTACLE   | >:  94.06   | >:  96.02   | v:  98.00   | \n",
      "-----------------------------------------------------------------------\n",
      "| ^:   0.00   |  OBSTACLE   | >:  96.02   | >:  98.00   | v: 100.00   | \n",
      "-----------------------------------------------------------------------\n",
      "| ^:   0.00   | >:  62.80   | ^:  94.06   |  OBSTACLE   |    GOAL     | \n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def visualize_q_table_as_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the Q-table as a grid with all action values for each state.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nDetailed Q-table Grid:\")\n",
    "    \n",
    "    # Header\n",
    "    header = \"   |\" + \"|\".join(f\"   ({i},{j})   \" for i in range(GRID_SIZE) for j in range(GRID_SIZE)) + \"|\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for action_idx, action_symbol in enumerate(action_symbols):\n",
    "        row = f\" {action_symbol} |\"\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i, j) == GOAL:\n",
    "                    cell = \"   GOAL    \"\n",
    "                elif (i, j) in OBSTACLES:\n",
    "                    cell = \" OBSTACLE  \"\n",
    "                else:\n",
    "                    q_value = q_table[i, j, action_idx]\n",
    "                    cell = f\" {q_value:9.2f} \"\n",
    "                row += cell + \"|\"\n",
    "        print(row)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "def visualize_best_actions_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the best action and its Q-value for each state in a grid.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nBest Actions Grid:\")\n",
    "    header = \"-\" * (14 * GRID_SIZE + 1)\n",
    "    print(header)\n",
    "\n",
    "    for i in range(GRID_SIZE):\n",
    "        row = \"| \"\n",
    "        for j in range(GRID_SIZE):\n",
    "            if (i, j) == GOAL:\n",
    "                cell = \"   GOAL    \"\n",
    "            elif (i, j) in OBSTACLES:\n",
    "                cell = \" OBSTACLE  \"\n",
    "            else:\n",
    "                best_action_idx = np.argmax(q_table[i, j])\n",
    "                best_q_value = q_table[i, j, best_action_idx]\n",
    "                cell = f\"{action_symbols[best_action_idx]}:{best_q_value:7.2f}  \"\n",
    "            row += cell + \" | \"\n",
    "        print(row)\n",
    "        print(header)\n",
    "\n",
    "# Visualize the Q-table as a grid\n",
    "visualize_q_table_as_grid(q_table)\n",
    "\n",
    "# Visualize the best actions and their Q-values in a grid\n",
    "visualize_best_actions_grid(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_projects_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
